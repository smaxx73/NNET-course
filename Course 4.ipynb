{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Network <div style=\"float:right\"><img class=\"w3-card-4\"\n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Logo_de_l%27acad%C3%A9mie_militaire_de_Saint-Cyr_Co%C3%ABtquidan.svg/1200px-Logo_de_l%27acad%C3%A9mie_militaire_de_Saint-Cyr_Co%C3%ABtquidan.svg.png\"\n",
    "                                                                          width=\"100px\" object-position=\"right top\"></div></h3>\n",
    "<div style=\"clear:both\"></div>\n",
    "\n",
    "<center>\n",
    "    <h1> Course 4 : Neural network : gradient learning rule and backpropagation </h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 {\n",
       "  border: 1.5px solid #333;\n",
       "  padding: 8px 12px;\n",
       "  background-color:#f0cfc0;\n",
       "  position: static;\n",
       "}  \n",
       "h2 {\n",
       "  padding: 8px 12px;\n",
       "  background-color:#f0cfc0;\n",
       "  position: static;\n",
       "}   \n",
       "h3 {\n",
       "  padding: 4px 8px;\n",
       "  background-color:#f0cfc0;\n",
       "  position: static;\n",
       "}   \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "h1 {\n",
    "  border: 1.5px solid #333;\n",
    "  padding: 8px 12px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}  \n",
    "h2 {\n",
    "  padding: 8px 12px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}   \n",
    "h3 {\n",
    "  padding: 4px 8px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}   \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, IFrame\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient for a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider a neural network with $n$ input $(x_1,...,x_n)$ and one output. That defines a fonction $F \\colon \\mathbb{R}^n \\to \\mathbb{R}$, $(x_1,...,x_n) \\mapsto F(x_1,...,x_n)$.\n",
    "\n",
    "Let $(a_1,...,a_m)$ be the set of weights of this neural network. Now we consider $(a_1,...,a_m)$ as variables and the function $\\widetilde{F} \\colon (a_1,...,a_m) \\mapsto \\widetilde{F}(a_1,...,a_m)$. \n",
    "\n",
    "In particular, we are interested in \n",
    "$$\\frac{\\partial \\widetilde{F}}{\\partial a_j}$$ for all $j \\in \\{1,...,m\\}$ and then the gradient of a cost function $E = (\\widetilde{F}-y_0)^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient formula"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We consider this part of neural network :\n",
    "    \n",
    "<img src = \"img/gradient1.png\"></img>\n",
    "\n",
    "and only one input in the red neuron with weight $a$ and activation function $g$.\n",
    "\n",
    "- $f$, $g$, $h$ are activation function ;\n",
    "- $a$ and $b$ are weights ;\n",
    "- $F$ is the function of the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By chain rule, we obtain :\n",
    "$$\\frac{\\partial F}{\\partial a} = f \\cdot \\frac{g'}{g}\\cdot b \\cdot \\frac{\\partial F}{\\partial b}$$\n",
    "\n",
    "With multiple output, it is quite similar :\n",
    "\n",
    "<img src = \"img/gradient2.png\"> </img>\n",
    "\n",
    "$$\\frac{\\partial F}{\\partial a} = \\sum_{i=1}^{\\ell} f \\cdot \\frac{g'}{g}\\cdot b_i \\cdot \\frac{\\partial F}{\\partial b_i}$$\n",
    "\n",
    "To understand the chain rule adapted to a neural network, think to this pattern :\n",
    "\n",
    "<img src=\"img/gradient_chainrule1.png\"></img>\n",
    "\n",
    "that leads to this :\n",
    "\n",
    "<img src=\"img/gradient_chainrule2.png\"></img>\n",
    "\n",
    "or this particular case :\n",
    "\n",
    "<img src=\"img/gradient_chainrule3.png\"></img>\n",
    "\n",
    "or this one with a bias : \n",
    "<img src=\"img/gradient_chainrule4.png\"></img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "This is a very simple neural network : \n",
    "\n",
    "<img src=\"img/gradient_chainrule_ex1.png\"></img>\n",
    "\n",
    "Using formulas above, calculate by hand the gradient of $F$ on value $(x,a,b,c) = (2,3,4,5)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer :** From right to left. \n",
    "\n",
    "First compute $\\frac{\\partial F}{\\partial c} = ...$\n",
    "\n",
    "then ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generalize this computation with a program which compute these partial derivatives for all values $(x,a,b,c)$ and any activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives(f,x,h):\n",
    "    df  = (f(x+h)-f(x))/(h)\n",
    "    return df\n",
    "\n",
    "def g1(x):\n",
    "    return ...\n",
    "\n",
    "def g2(x):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([2,3,4,5])\n",
    "eps = 1e-8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dFc = ...\n",
    "print(dFc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dFa = ...\n",
    "print(dFa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dFb = ...\n",
    "print(dFb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dFx = ...\n",
    "print(dFx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradF = dFa,dFb,dFc\n",
    "print(gradF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General algorithm\n",
    "\n",
    "<img src=\"img/gradient_chainrule_general.png\"> </img>\n",
    "\n",
    "From right to left :\n",
    "\n",
    "1. Output neuron : $\\frac{\\partial F}{\\partial g_n} = 1$ ;\n",
    "2. By induction : assume that $\\frac{\\partial F}{\\partial g_{i+1}}$ is already computed, then $\\frac{\\partial F}{\\partial g_{i}} = \\frac{\\partial F}{\\partial g_{i+1}} \\cdot a_{i+1} \\cdot g'_{i+1}$ and \n",
    "$$\\frac{\\partial F}{\\partial a_i} = \\frac{\\partial F}{\\partial g_{i}} \\cdot g_{i-1} \\cdot g_i'$$\n",
    "\n",
    "All these formulas are supposed to be evaluated at a point $(x_1,x_2,...)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "#### Algorithm : \n",
    "It gives a sequence $x_0,x_1,...$ defined by this algorithm :\n",
    "1. Compute gradient : $\\nabla f(x_k)$ ;\n",
    "2. Stopping criteria : $||\\nabla f(x_k)||<\\varepsilon$ ;\n",
    "3. Choose a step value $\\alpha_k >0$ ;\n",
    "4. Iteration : $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from descent import *\n",
    "from descente_stochastique import *\n",
    "from descente_lot import *\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def exemple1():\n",
    "    # 2-var function\n",
    "    def f(x, y):\n",
    "        return x**2 + 3*y**2\n",
    "    \n",
    "    # handmade gradient\n",
    "    def grad_f(x, y):\n",
    "        g = [2*x, 6*y]\n",
    "        return np.array(g)\n",
    "\n",
    "    # Test\n",
    "    print(\"--- gradient descent ---\")\n",
    "    X0 = np.array([2, 1])    \n",
    "    my_step = 0.2\n",
    "    X0 = np.array([-1, -1])    \n",
    "    my_step = 0.1    \n",
    "    display_descent(f, grad_f, X0, delta=my_step, nmax = 21)\n",
    "    graphic_descent_2var_2d(f, grad_f, X0, delta=my_step, nmax = 10, zone = (-2.5,2.5,-1.5,1.5) ) \n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exemple1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First example with one neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We want to separate the plane according to these two sets of points : \n",
    "\n",
    "blue circles : (0, 3), (1, 1.5), (1, 4), (1.5, 2.5), (2, 2.5), (3, 3.5), (3.5, 3.25), (4, 3), (4, 4), (5, 4)\n",
    "\n",
    "red squares : (1, 1), (2, 0.5), (2, 2), (3, 1.5), (3, 2.75), (4, 1), (4, 2.5), (4.5, 3), (5, 1), (5, 2.25).\n",
    "\n",
    "with a single neuron perceptron : \n",
    "\n",
    "<img src=\"img/propagation_ex1.png\"></img>\n",
    "\n",
    "The activation function is the sigmoid function.\n",
    "\n",
    "<img src=\"img/propagation_ex1_1.png\"></img>\n",
    "\n",
    "The cost function is \n",
    "$$E(a,b,c) = \\frac{1}{N}\\sum_{i=1}^N E_i(a,b,c)$$\n",
    "where $E_i = (F(x_i,y_i)-t_i)^2$ and $t_i=1$ when $(x_i,y_i)$ is a red square, $t_i=0$ when $(x_i,y_i)$ is a blue circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data : training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_points = [(0, 3), (1, 1.5), (1, 4), (1.5, 2.5), (2, 2.5), (3, 3.5), (3.5, 3.25), (4, 3), (4, 4), (5, 4)]\n",
    "red_points = [(1, 1), (2, 0.5), (2, 2), (3, 1.5), (3, 2.75), (4, 1), (4, 2.5), (4.5, 3), (5, 1), (5, 2.25)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = []\n",
    "points = []\n",
    "for x,y in blue_points:\n",
    "    target.append(0)\n",
    "    points.append((x,y))\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_points:\n",
    "    target.append(1)\n",
    "    points.append((x,y))\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Gradient descent\n",
    "\n",
    "We want to find the best weights $W=(a,b,c)$ by iteration : initialize $W_0 = (a_0,b_0,c_0)$, for example $W_0 = (0,1,-2)$ and fix a step value $\\delta = 1$. The sequence of weights is define by\n",
    "$$W_{k+1} = W_k - \\delta \\cdot \\nabla E(W_k)$$\n",
    "\n",
    "The local error is :\n",
    "$$E_i(a,b,c) = (\\sigma(ax_i+by_i+c)-t_i)^2$$\n",
    "\n",
    "and the error is the sum of local error where $N$ is the number of points in the training set :\n",
    "\n",
    "$$E(a,b,c) = \\frac{1}{N}\\sum_{i=1}^N E_i(a,b,c)$$\n",
    "\n",
    "Notice that $\\sigma' = \\sigma(1-\\sigma)$ so that \n",
    "$$\\frac{\\partial E_i}{\\partial a}(x_i,y_i) = 2x_i \\sigma_i(1-\\sigma_i)(\\sigma_i-t_i)$$\n",
    "$$\\frac{\\partial E_i}{\\partial b}(x_i,y_i) = 2y_i \\sigma_i(1-\\sigma_i)(\\sigma_i-t_i)$$\n",
    "$$\\frac{\\partial E_i}{\\partial c}(x_i,y_i) = 2 \\sigma_i(1-\\sigma_i)(\\sigma_i-t_i)$$\n",
    "where $\\sigma_i = \\sigma(ax_i+by_i+c)$.\n",
    "\n",
    "Finaly, $$\\nabla E(W_k) = \\frac{1}{N}\\sum_{i=1}^N \\left[\\frac{\\partial E_i}{\\partial a},\\frac{\\partial E_i}{\\partial b},\\frac{\\partial E_i}{\\partial c}\\right]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(y):\n",
    "    return 1/(1+np.exp(-y))\n",
    "\n",
    "def p(y):\n",
    "    return y*(1-y)\n",
    "\n",
    "def dsigmoid(y):\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = np.array([0,1,-2])   #init weights\n",
    "\n",
    "#gradient of local error :\n",
    "def gradE(W,x,y,t):\n",
    "    ...\n",
    "    return gradEa,gradEb,gradEc\n",
    "\n",
    "#local error :\n",
    "def E(x,y,W,t):\n",
    "    return (sigmoid(np.dot(W[:2],np.array([x,y]))+W[2])-t)**2\n",
    "\n",
    "#gradient of cost function :\n",
    "def gradE_total(W):\n",
    "    g = np.array([0,0,0])\n",
    "    i = 0\n",
    "    for (x,y) in points:\n",
    "        g = np.array(gradE(..., ..., ..., target[i])) + g\n",
    "        i += 1\n",
    "    g=g/(i)\n",
    "    return g\n",
    "#sum of local errors\n",
    "def E_total(W):\n",
    "    e = 0.0\n",
    "    i = 0\n",
    "    for (x,y) in points:\n",
    "        e = E(..., ..., ..., ...) + e\n",
    "        i += 1\n",
    "    e = e/(i)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init\n",
    "W = np.array([0,1,-2])\n",
    "epoch = 1000  #change number of iterations\n",
    "delta = 1\n",
    "error = []\n",
    "\n",
    "# GRADIENT DESCENT\n",
    "for i in range(epoch):\n",
    "    W = ...\n",
    "    error.append(E_total(W))\n",
    "\n",
    "print('(a,b,c) = '+str(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in blue_points:\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_points:\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "h = np.array([-0.1,5.1])\n",
    "v = ...  #equation of boundary line\n",
    "plt.fill_between(h,v,4.1,color=\"blue\",alpha=0.1)\n",
    "plt.fill_between(h,v,0,color=\"red\",alpha=0.1)\n",
    "\n",
    "plt.plot(h,v,'-')\n",
    "plt.title('epoch= '+str(epoch))\n",
    "plt.show()\n",
    "print('Error = '+ str(E_total(W)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epoch),error)\n",
    "plt.title('Evolution of error up to '+str(epoch)+ ' iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second example : approximate a step function\n",
    "\n",
    "We want to find a neural network realizing a function $F$ such that :\n",
    "* if $x \\in [0;2] \\cup [6;8]$, $F(x)=0$ ;\n",
    "* if $x \\in [3;5]$, $F(x) = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data : training set\n",
    "We consider 10 blue circles on $[0;2]$, 10 blue circles on $[6;8]$ and 10 red squares on $[3;5]$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blue_circles = []\n",
    "target = []\n",
    "X = np.linspace(0,2,10)\n",
    "\n",
    "for x in X:\n",
    "    blue_circles.append((x,0))\n",
    "\n",
    "X = np.linspace(6,8,10)\n",
    "\n",
    "for x in X:\n",
    "    blue_circles.append((x,0))\n",
    "\n",
    "red_squares = []\n",
    "\n",
    "X = np.linspace(3,5,10)\n",
    "\n",
    "for x in X:\n",
    "    red_squares.append((x,1))\n",
    "\n",
    "for x,y in blue_circles:\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_squares:\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "\n",
    "points = blue_circles+red_squares\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose this architecture of neural network :\n",
    "\n",
    "<img src=\"img/propagation_ex2.png\"></img>\n",
    "\n",
    "with the sigmoid activation function. \n",
    "\n",
    "Thus, we are looking for 7 weights $(a_1,a_2,...,a_7)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#init weights\n",
    "W = [0.0,1.0,0.0,-1.0,1.0,1.0,-1.0]\n",
    "\n",
    "#output of this neural network\n",
    "def F(x,W):\n",
    "    layer1 = ...\n",
    "    layer2 = ...\n",
    "    return ...\n",
    "#gradient of F\n",
    "def gradF(x,W):\n",
    "    layer1 = ...\n",
    "    layer2 = ...\n",
    "    ...\n",
    "    grad = [g1,g2,g3,g4,g5,g6,g7]\n",
    "    return grad\n",
    "\n",
    "#gradient of local error\n",
    "def gradE(W,x,y):\n",
    "    output = F(x,W)\n",
    "    grad = [... for g in gradF(x,W)]\n",
    "    return grad\n",
    "\n",
    "# local error\n",
    "def E(W,x,t):\n",
    "    return ...\n",
    "\n",
    "#gradient of cost function\n",
    "def gradE_total(W):\n",
    "    g = np.array([0,0,0,0,0,0,0])\n",
    "    i=0\n",
    "    for (x,y) in points:\n",
    "        g = np.array(gradE(W,...,...)) + g\n",
    "        i+=1\n",
    "    return g/i\n",
    "\n",
    "def E_total(W):\n",
    "    e = 0.0\n",
    "    i=0\n",
    "    for (x,y) in points:\n",
    "        e = E(W,x,y) + e\n",
    "        i+=1\n",
    "    e = e/(i)\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch = 5000  #change number of iterations\n",
    "delta = 1\n",
    "error = []\n",
    "\n",
    "# GRADIENT DESCENT\n",
    "for i in range(epoch):\n",
    "    W = ...\n",
    "    error.append(E_total(W))\n",
    "\n",
    "print('Weights after training : ' + str(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(epoch),error)\n",
    "plt.title('Evolution of error up to '+str(epoch)+ ' iterations')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in blue_circles:\n",
    "    plt.scatter(x,y,color='blue')\n",
    "for x,y in red_squares:\n",
    "    plt.scatter(x,y,color='red',marker='s')\n",
    "X = np.linspace(0,8,200)\n",
    "Y = [F(x,W) for x in X]\n",
    "plt.plot(X,Y,color='purple',linewidth=4)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
