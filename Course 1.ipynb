{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Network <div style=\"float:right\"><img class=\"w3-card-4\"\n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Logo_de_l%27acad%C3%A9mie_militaire_de_Saint-Cyr_Co%C3%ABtquidan.svg/1200px-Logo_de_l%27acad%C3%A9mie_militaire_de_Saint-Cyr_Co%C3%ABtquidan.svg.png\"\n",
    "                                                                          width=\"100px\" object-position=\"right top\"></div></h3>\n",
    "<div style=\"clear:both\"></div>\n",
    "\n",
    "<center>\n",
    "    <h1> Course 1 : prerequisite in python and mathematics  </h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "h1 {\n",
    "  border: 1.5px solid #333;\n",
    "  padding: 8px 12px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}  \n",
    "h2 {\n",
    "  padding: 8px 12px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}   \n",
    "h3 {\n",
    "  padding: 4px 8px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}   \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from IPython.display import Latex\n",
    "\n",
    "from pylab import *\n",
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Derivative\n",
    "\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Let $f \\colon I \\to \\mathbb{R}$ a function. If it exists, the derivative of $f$ in $x_0 \\in I$ is the number \n",
    "    $$\\displaystyle \\lim_{x\\rightarrow x_0}\\frac{f(x)-f(x_0)}{x-x_0} = \\lim_{h\\rightarrow 0}\\frac{f(x_0 + h)-f(x_0)}{h} = f'(x_0) = \\frac{df}{dx}(x)$$\n",
    "An another way to see that is by considering the Taylor expansion :\n",
    "\t$$ \\forall h, \\ \\ f(x_0 + h) = f(x_0) + f'(x_0) \\cdot h + h \\varepsilon(h) \\ \\ \n",
    "\t\t\\text{ with } \\displaystyle\\lim_{h \\rightarrow 0} \\varepsilon(h) = 0 $$\n",
    "        \n",
    "Thus, $f'(x_0) \\approx \\dfrac{f(x_0 + h)-f(x_0)}{h}$ if $h$ is small.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example :\n",
    "Let $f(x) = \\mathrm{exp}(x)$. We have $\\frac{f(0 + h)-f(0)}{h} = \\frac{e^h - 1}{h} \\xrightarrow[h \\to 0]{} 1$ so we prove that $f'(0) = ...$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "<div class=\"alert alert-dark\" role=\"alert\">\n",
    "    Write a python function which computes an approximation of the derivative of a given function.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def derivatives(f, x, h):\n",
    "    df  = ...\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and test this function to compute an approximation of $\\sin'(0)$ (you know the exact answer !) :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivatives(math.sin, ..., 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens when $h = 0.001$ is getting smaller or larger ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "<div class=\"alert alert-dark\" role=\"alert\">\n",
    "Fill in the blank in the following code so that the plot shows the graph of a function $f$ and its derivative.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Tip : consider the following code and understand what happens:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = linspace(1, 10, 10)\n",
    "print(z)\n",
    "print(z[1:])\n",
    "print(z[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 101  #number of points\n",
    "x = linspace(..., ..., ...)  #modelize the interval [-5,5] with N points for example\n",
    "y = cos(x) # you can change the function f here\n",
    "h = ... # step of discretization\n",
    "\n",
    "# vector of values of the derivative\n",
    "yp = ...\n",
    "\n",
    "plot(x, y, label=\"f(x)\")\n",
    "plot(x[:-1], yp, label=\"f'(x)\")\n",
    "\n",
    "legend()\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to compute the exact derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usual formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://en.wikipedia.org/wiki/Derivative#Rules_of_computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\t\n",
    "\n",
    "Let two real differentiable functions $f \\colon x \\mapsto f(x)$ and $x \\colon t \\mapsto x(t)$.\n",
    "The function   $g \\colon t  \\mapsto  x(t)  \\mapsto f(x(t)) $ is differentiable and \n",
    "\n",
    "$$ g'(t) = f'(x(t)) \\cdot x'(t) $$\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariables function and optimization\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition\n",
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "\tA real-valued function of n real variables is a function $f \\colon \\mathbb{R}^n \\to \\mathbb{R}^m$ which maps $X = (x_1,...,x_n) \\in \\mathbb{R}^n$ to $f(X) \\in \\mathbb{R}^m$. It is also called a n-ary function.\n",
    "\t\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/61/Real_function_of_two_real_variables.svg/800px-Real_function_of_two_real_variables.svg.png\"\n",
    "     alt=\"picture of multiple variables function\"\n",
    "     style=\"float: left; margin-right: 10px; width: 300px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Let $f$ a n-ary function. \tThe partial derivative of $f$ in the direction xi at the point (a1, ..., an) is defined to be: \n",
    "\t$$\\frac{\\partial f}{\\partial x_i}(a_1, \\ldots, a_n) = \\lim_{h \\to 0}\\frac{f(a_1, \\ldots, a_i+h,\\ldots,a_n) - f(a_1,\\ldots, a_i, \\dots,a_n)}{h}.$$\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example\n",
    "Le $f \\colon (x,y) \\mapsto x^2 + 2xy - 2y^2$. Then \n",
    "$$\\frac{\\partial f}{\\partial x}(x,y) = ... \\qquad ; \\qquad \\frac{\\partial f}{\\partial y}(x,y) = ... $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient\n",
    "<p>Let <span class=\"math inline\">\\(f \\colon \\mathbf{R}^n \\to \\mathbf{R}\\)</span>, its gradient <span class=\"math inline\">\\(\\nabla f \\colon \\mathbf{R}^n \\to \\mathbf{R}^n\\)</span> is defined at the point <span class=\"math inline\">\\(p = (x_1,\\ldots,x_n)\\)</span> in <em>n-</em>dimensional space as the vector: </p>\n",
    "\n",
    "<p><span class=\"math inline\">\\(\\nabla f(p) = \\begin{bmatrix}\\frac{\\partial f}{\\partial x_1}(p) \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n}(p) \\end{bmatrix}.\\)</span></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise : \n",
    "Define a function which computes an approximated gradient on a point $x \\in \\mathbb{R}^n$. Let $\\varepsilon = 10^{-8}$ be the precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1e-8\n",
    "def gradientApprox(f,x):\n",
    "    fx = f(x)\n",
    "    n = size(x)\n",
    "    gra = zeros(n)\n",
    "    for i in range(n):\n",
    "        ...\n",
    "    \n",
    "    return gra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " and test it on the function $f(x,y) = 1 - \\frac{1}{1+3x^2+y^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa(X):  #X in R^2\n",
    "    return 1-1/(1+3*X[0]**2+X[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = array([1,1])\n",
    "print(gradientApprox(fa,X))  #compute the gradient of fa in (1,1) : should have (0.24 , 0.08) with these datas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st case : $\\mathbb{R} \\to \\mathbb{R}^2 \\to \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f \\colon (x,y) \\mapsto f(x,y)$, $x \\colon t \\mapsto x(t)$ and $y \\colon t \\mapsto y(t)$.\n",
    "\n",
    "Then $g \\colon t \\mapsto (x(t),y(t)) \\mapsto f(x(t),y(t))$ is differentiable and \n",
    "$$g'(t) = \\frac{\\partial f}{\\partial x}(x(t),y(t)) \\cdot x'(t) + \\frac{\\partial f}{\\partial y}(x(t),y(t)) \\cdot y'(t)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd case : $\\mathbb{R}^2 \\to \\mathbb{R}^2 \\to \\mathbb{R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f \\colon (x,y) \\mapsto f(x,y)$, $x \\colon (t,u) \\mapsto x(t,u)$ and $y \\colon (t,u) \\mapsto y(t,u)$.\n",
    "\n",
    "Then $g \\colon (t,u) \\mapsto (x(t,u),y(t,u)) \\mapsto f(x(t,u),y(t,u))$ is differentiable and \n",
    "$$\\frac{\\partial g}{\\partial t}(t,u) = \\frac{\\partial f}{\\partial x}(x(t,u),y(t,u)) \\cdot \\frac{\\partial x}{\\partial t}(t,u) + \\frac{\\partial f}{\\partial y}(x(t,u),y(t,u)) \\cdot \\frac{\\partial y}{\\partial t}(t,u)$$\n",
    "$$\\frac{\\partial g}{\\partial u}(t,u) = \\frac{\\partial f}{\\partial x}(x(t,u),y(t,u)) \\cdot \\frac{\\partial x}{\\partial u}(t,u) + \\frac{\\partial f}{\\partial y}(x(t,u),y(t,u)) \\cdot \\frac{\\partial y}{\\partial u}(t,u)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Let $f$ a function defined on a domain $D \\subset \\mathbb{R}^2$ by \t$$f(x,y)= e^{\\sqrt{x}y-x\\sqrt{y}}  $$ and  two functions $x$ and $y$ defined on $\\mathbb{R}$ by\t\t$$x(t)=3t \\quad y(t)=t(1-t)$$\n",
    "\n",
    "Let $G(t) = f(x(t),y(t)$. Calculate by hand $G'(t)$ for all $t$ where $G$ is differentiable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "Let <span class=\"math inline\">\\(f \\colon \\mathbf{R}^n \\to \\mathbf{R}\\)</span>, a critical point is a vector $p = (x_1,\\ldots,x_n)$ such that $\\nabla f(p) = 0$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\t\n",
    "Let <span class=\"math inline\">\\(f \\colon \\mathbf{R}^n \\to \\mathbf{R}\\)</span>, if $f$ has a local extremum at $p$ then $p$ is a critical point.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If $n=2$, considering the second derivatives allows to classify critical points :\n",
    "\n",
    "<img src=\"https://math.libretexts.org/@api/deki/files/11440/imageedit_9_5374076016.png?revision=1\"\n",
    "     alt=\"picture of multiple variables function\"\n",
    "     style=\"float: left; margin-right: 10px; width: 800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More details here : \n",
    "https://math.libretexts.org/Courses/Monroe_Community_College/MTH_212_Calculus_III/Chapter_13%3A_Functions_of_Multiple_Variables_and_Partial_Derivatives/13.8%3A_Optimization_of_Functions_of_Several_Variables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting two variables functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Let 3 functions :\n",
    "* $f_1(x,y) = \\sqrt{1-\\left(2-\\sqrt{x^2+y^2}\\right)^2 }$\n",
    "* $f_2(x,y) = x^2 + \\frac{10}{y^2+1}$\n",
    "* $f_3(x,y) = 5(x^2+y^2)e^{-x^2-y^2}$\n",
    "\n",
    "By filling the following code, plot each function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x, y):\n",
    "    z = sqrt(1 - (2 - sqrt(x**2 + y**2))**2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "min = -3\n",
    "max = 3\n",
    "VX = linspace(min, max ,n)\n",
    "VY = linspace(min, max ,n)\n",
    "X,Y = meshgrid(VX, VY)\n",
    "Z = f1(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = axes(projection = '3d')\n",
    "ax.plot_wireframe(X, Y, Z, alpha=0.7)\n",
    "ax.view_init(70, -160)\n",
    "title('Half donut')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(x, y):\n",
    "    z = ...\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "min = ...\n",
    "max = ...\n",
    "VX = ...\n",
    "VY = ...\n",
    "X,Y = meshgrid(VX,VY)\n",
    "Z = f2(X,Y)\n",
    "ax = axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z)\n",
    "ax.view_init(30, -130)\n",
    "title('Inverted slope')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f3(x,y):\n",
    "    z=5*(x**2 + y**2)*exp(-x**2-y**2)\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "min=...\n",
    "max=...\n",
    "VX = ...\n",
    "VY = ...\n",
    "X,Y = meshgrid(VX,VY)\n",
    "Z = f3(X,Y)\n",
    "ax = axes(projection='3d')\n",
    "ax.plot_surface(X, Y, Z,cmap='hot')\n",
    "ax.view_init(50, -130)\n",
    "title('Vulcania')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $f(x,y)=xe^{-x^2-y^2}$. By plotting this function on the square $[-4;4]^2$, do you think that it admits local extrema ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    z = x * exp(...)\n",
    "    return z\n",
    "\n",
    "...\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By plotting some level sets, identify critical points of this function :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_levels = linspace(-0.5,0.5,15)\n",
    "levels=contour(X,Y,Z,my_levels,cmap='viridis')\n",
    "grid()\n",
    "clabel(levels,inline=1,fontsize=8)\n",
    "scatter([1/sqrt(2),-1/sqrt(2)],[0,0])\n",
    "axis('equal')\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function which compute the exact gradient of this function $f$ (you can do the symbolic calculation by hand). Verify that critical points you found above really are critical points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dfx(x,y):\n",
    "    z=...\n",
    "    return z\n",
    "\n",
    "def dfy(x,y):\n",
    "    z=...\n",
    "    return z\n",
    "\n",
    "def gradientf(x,y):\n",
    "    return ...\n",
    "\n",
    "gradientf(...,...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a function to visualize level sets in a chosen square :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def levelset(min,max):\n",
    "    n=100\n",
    "    VX = linspace(min, max, n)\n",
    "    VY = linspace(min, max, n)\n",
    "    X,Y = meshgrid(VX, VY)\n",
    "    Z = f(X,Y)\n",
    "    my_levels = linspace(-0.4,0.4,50)  #display 50 levels between -0.4 and 0.4\n",
    "    levels=contour(X,Y,Z,my_levels,cmap='viridis',alpha=0.6)\n",
    "    grid()\n",
    "    clabel(levels,inline=1,fontsize=8)\n",
    "    scatter([1/sqrt(2),-1/sqrt(2)],[0,0])\n",
    "    axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levelset(-2,2)\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you observe when you plot some gradient vectors on levelsets ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gradient(x,y):\n",
    "    M=array([x,y])  #define a point M with its coordinates\n",
    "    arrow(M[0],M[1],gradientf(x,y)[0],gradientf(x,y)[1],width=0.04,head_width=0.08,color='red') #plot an arrow from point M, oriented by the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "levelset(...,...)\n",
    "plot_gradient(-1,1)\n",
    "plot_gradient(1,1)\n",
    "plot_gradient(1.2,1.3)\n",
    "plot_gradient(0.8,0.5)\n",
    "plot_gradient(1.4,1.8)\n",
    "plot_gradient(-0.7,-0.1)\n",
    "plot_gradient(-0.7,-0.5)\n",
    "plot_gradient(-1.5,-1.5)\n",
    "...\n",
    "show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These experiments above are a first approch of an algorithm which gives a sequence converging to local extrema. A well known algorithm is the gradient descent algorithm that we will use to make neural networks learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Algorithm : \n",
    "It gives a sequence $x_0,x_1,...$ defined by this algorithm :\n",
    "1. Compute gradient : $\\nabla f(x_k)$ ;\n",
    "2. Stopping criteria : $||\\nabla f(x_k)||<\\varepsilon$ ;\n",
    "3. Choose a step $\\alpha_k >0$ ;\n",
    "4. Iteration : $x_{k+1} = x_k - \\alpha_k \\nabla f(x_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example with fixed step size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientFixedStep(f, X0, step, eps, N=200):\n",
    "    lX = ...\n",
    "    ...\n",
    "    grad = gradientApprox(f,X0)\n",
    "    n=0\n",
    "    while norm(grad)>eps and ...:\n",
    "        ...\n",
    "        ...\n",
    "        ...\n",
    "        lX.append(...)\n",
    "    return lX # return a vector containing the sequence of points converging toward a local extremum of function f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We test this method with function fa :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fa(X):  #X in R^2\n",
    "    return 1-1/(1+3*X[0]**2+X[1]**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = gradientFixedStep(fa, [1,1], 0.2, 1e-5, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lx0 = [X[0] for X in L]\n",
    "Lx1 = [X[1] for X in L]\n",
    "n = 100\n",
    "min = -2\n",
    "max = 2\n",
    "VX = linspace(min, max, n)\n",
    "VY = linspace(min, max, n)\n",
    "Z=array([[fa(array([x0, x1])) for x0 in VX] for x1 in VY])\n",
    "contour(VX,VY,Z,12)\n",
    "plot(Lx0, Lx1, \"-ro\")\n",
    "axis('scaled')\n",
    "show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
