{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Neural Network <div style=\"float:right\"><img class=\"w3-card-4\"\n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/b/b6/Logo_de_l%27acad%C3%A9mie_militaire_de_Saint-Cyr_Co%C3%ABtquidan.svg/1200px-Logo_de_l%27acad%C3%A9mie_militaire_de_Saint-Cyr_Co%C3%ABtquidan.svg.png\"\n",
    "                                                                          width=\"100px\" object-position=\"right top\"></div></h3>\n",
    "<div style=\"clear:both\"></div>\n",
    "\n",
    "<center>\n",
    "    <h1> Course 3 : Neural network : first examples and theory </h1>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>\n",
       "h1 {\n",
       "  border: 1.5px solid #333;\n",
       "  padding: 8px 12px;\n",
       "  background-color:#f0cfc0;\n",
       "  position: static;\n",
       "}  \n",
       "h2 {\n",
       "  padding: 8px 12px;\n",
       "  background-color:#f0cfc0;\n",
       "  position: static;\n",
       "}   \n",
       "h3 {\n",
       "  padding: 4px 8px;\n",
       "  background-color:#f0cfc0;\n",
       "  position: static;\n",
       "}   \n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<style>\n",
    "h1 {\n",
    "  border: 1.5px solid #333;\n",
    "  padding: 8px 12px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}  \n",
    "h2 {\n",
    "  padding: 8px 12px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}   \n",
    "h3 {\n",
    "  padding: 4px 8px;\n",
    "  background-color:#f0cfc0;\n",
    "  position: static;\n",
    "}   \n",
    "</style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, IFrame\n",
    "from IPython.core.display import HTML\n",
    "from IPython.display import Latex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, given some input $(x_1,...,x_n)$, we organise multiple neurons in **layers**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://www.researchgate.net/profile/Facundo_Bre/publication/321259051/figure/fig1/AS:614329250496529@1523478915726/Artificial-neural-network-architecture-ANN-i-h-1-h-2-h-n-o.png\"> </img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 0 : a neural network with 2 layers : 2 neurons (linear perceptron with ReLu activation function) on the 1st layer, 1 neuron (affine perceptron with Heavyside activation function) on the 2nd layer.\n",
    "\n",
    "<img src=\"img/neural_layer_ex1.png\"> </img>\n",
    "\n",
    "**Exercise** : check that output is $1$ if input is $(4,7)$ by filling the code.\n",
    "\n",
    "_Tip_ : we can use a matrix multiplication: in Python, $A \\times B$ is computed with this command : ```np.dot(A,B)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLu(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return y\n",
    "\n",
    "def H(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([...,...])\n",
    "W1 = np.array([[...,...],[...,...]])\n",
    "Y1 = ...\n",
    "\n",
    "W2 = np.array([...,...])\n",
    "b2 = ...\n",
    "Y2 = ...\n",
    "\n",
    "print(Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "<img src=\"img/neural_layer_ex2.png\"> </img>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st layer, 1st neuron :\n",
    "Is active if $-x+3y \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "y=np.array([t/3 for t in x])\n",
    "plt.plot(x,y,'k--')\n",
    "plt.fill_between(x,y,np.max(y)+2,color=\"green\",alpha=0.5)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1st layer, 2nd neuron :\n",
    "Is active if $2x+y \\geq 0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "z=np.array([... for t in x]) #write here something which defines the boundary of the half plane defined by this neuron\n",
    "plt.plot(x,z,'k--')\n",
    "plt.fill_between(x,z,np.max(z),color=\"green\",alpha=0.5)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2nd layer, one neuron :\n",
    "\n",
    "The output neuron realize the boolean function ```x ... y``` (c.f. previous course)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### result of two layers :\n",
    "\n",
    "The neural network has output equal to $1$ on the __... ... ...__ of the two half planes where neurones of 1st layer are equal to $1$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "y=np.array([... for t in x])\n",
    "z=np.array([... for t in x])\n",
    "plt.plot(x,y,'k--')\n",
    "plt.plot(x,z,'k--')\n",
    "plt.fill_between(x,y,np.max(y)+2,color=\"green\",alpha=0.3)\n",
    "plt.fill_between(x,z,np.max(z),color=\"green\",alpha=0.3)\n",
    "plt.fill_between(x,z,np.max(z), where=y<z,color=\"red\",alpha=1)\n",
    "plt.fill_between(x,y,np.max(z), where=y>z,color=\"red\",alpha=1)\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "<img src=\"img/neural_layer_ex3.png\"> </img>\n",
    "\n",
    "Comments : 1st layer is the same than before, output neuron is the ```... ...```neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.linspace(-6,6,300)\n",
    "y=np.array([t/3 for t in x])\n",
    "z=np.array([-2*t for t in x])\n",
    "plt.plot(x,y,'k--')\n",
    "plt.plot(x,z,'k--')\n",
    "plt.fill_between(x,y,np.max(y)+2,color=\"red\",alpha=1)\n",
    "plt.fill_between(x,z,np.max(z),color=\"red\",alpha=1)\n",
    "#plt.fill_between(x,z,np.max(z), where=y<z,color=\"red\",alpha=0.5)\n",
    "#plt.fill_between(x,y,np.max(z), where=y>z,color=\"red\",alpha=0.5)\n",
    "plt.axis('equal')\n",
    "plt.axis([-5,5,-3,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network has output equal to $1$ on the __...__ of the two half planes where neurones of 1st layer are equal to $1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise :\n",
    "Find a neural network whose output is 1 on the red area, 0 otherwise.\n",
    "<img src=\"img/neural_layer_ex4.png\"> </img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def H(y):\n",
    "    if y<0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "def nnetwork(x,y):\n",
    "    X = np.array([x,y])\n",
    "    W1 = np.array([[...,...],[...,...],[...,...]])\n",
    "    B1 = np.array([...,...,...])\n",
    "    W2 = np.array([[...,...,...]])\n",
    "    B2 = ...\n",
    "    layer1 = np.dot(W1,X)+B1 #first computation\n",
    "    layer1 = np.array([H(t) for t in layer1]) #then compose by the activation function\n",
    "    layer2 = np.dot(...,...)+B2 #second computation\n",
    "    layer2 = ...          #then compose by the activation function\n",
    "    output = layer2\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check your network here by putting some color on the 2d-plane !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n=100\n",
    "xmin = -6\n",
    "xmax = 5\n",
    "ymin = -5\n",
    "ymax = 5\n",
    "X = np.linspace(xmin,xmax,n)\n",
    "Y = np.linspace(ymin,ymax,n)\n",
    "for x in X:\n",
    "    for y in Y:\n",
    "        if nnetwork(x,y) == 1:\n",
    "            plt.plot(x,y,'.r')\n",
    "plt.axis('equal')\n",
    "plt.axis([xmin,xmax,ymin,ymax])\n",
    "xtick=np.linspace(xmin,xmax,xmax-xmin+1)\n",
    "ytick=np.linspace(ymin,ymax,ymax-ymin+1)\n",
    "plt.xticks(xtick)\n",
    "plt.yticks(ytick)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to realize ```XOR``` ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise : \n",
    "We have seen last course that one neuron is not sufficient to realize the operation ```XOR```. Find a neural network with two layers that realize the operation ```XOR```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realizable sets : definition and properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\" role=\"alert\">\n",
    "A set $A$ is NN-realizable if it exists a neural network whose output is equal to $1$ in $A$, $0$ outside of $A$.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "Every convex $n$-polygon in $\\mathbb{R}^2$ is NN-realizable with $n+1$ neurons. \n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "If $A$ et $B$ are two NN-realizable sets then :\n",
    "   <ol> \n",
    "       <li> $A \\cup B$ is NN-realizable ;</li>\n",
    "    <li> $A \\cap B$ is NN-realizable ;</li>  \n",
    "    <li> $\\overline{A}$ is NN-realizable ;</li>\n",
    "    <li> $A \\backslash B$ is NN-realizable.</li>\n",
    "    </ol>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-danger\" role=\"alert\">\n",
    "Every polygon in $\\mathbb{R}^2$ is NN-realizable. Then, every Jordan curve (simpe closed curve) can be approximated by a neural network. \n",
    "   </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Universal approximation theorem \n",
    "Goal : approximate every continuous function $\\mathbb{R} \\to \\mathbb{R}$ by a neural network. More precisely, let $f \\colon [a;b] \\to \\mathbb{R}$ : we want to find a neural network whose output $F(x) \\approx f(x)$ for all $x \\in [a;b]$. To do this, assume that the output neuron has the identity $x \\mapsto x$ activation function. Other neurons have Heaviside activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Heaviside step functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First trivial case :\n",
    "<img src=\"img/step_function1.png\"></img>\n",
    "\n",
    "Its is easy to shift the step on the left :\n",
    "<img src=\"img/step_function2.png\"></img>\n",
    "\n",
    "or and the right :\n",
    "<img src=\"img/step_function3.png\"></img>\n",
    "\n",
    "step down :\n",
    "<img src=\"img/step_function4.png\"></img>\n",
    "<img src=\"img/step_function5.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rectangular functions\n",
    "Just add two Heaviside step functions !\n",
    "\n",
    "**Exercise** : find the suitable weights below.\n",
    "<img src=\"img/rect_function_withoutsol.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step functions\n",
    "Just add some rectangular functions ! Be careful if the rectangles ar contiguous.\n",
    "<img src=\"img/step_function6.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous functions\n",
    "Finally, notice that every continuous function $[a;b] \\to \\mathbb{R}$ can be uniformly approximated by a step function.\n",
    "\n",
    "<img src=\"img/approx_function.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In higher dimension :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise :\n",
    "Find a neural network that performs this 2-dimensionnal function :\n",
    "\n",
    "<img src=\"img/step_function7.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Answer : \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training rule for a 1-layer network\n",
    "\n",
    "Let $W$ be the weight matrix : each row is corresponding to a neuron of the layer. Let $b$ be the column matrix of bias : each row is corresponding to a neuron of the layer. \n",
    "\n",
    "Let $e$ be the column matrix of error, then the training rule is:\n",
    "\n",
    "$$W^{new} = W^{old} + e \\times x^T$$\n",
    "$$b^{new} = b^{old} + e$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. How to pick an architecture\n",
    "\n",
    "Problem specifications help define the network in the following ways:\n",
    "1. Number of network inputs = number of problem inputs\n",
    "2. Number of neurons in output layer = number of problem outputs\n",
    "3. Output layer transfer function choice at least partly determined by\n",
    "problem specification of the outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "\n",
    "A single-layer neural network is to have six inputs and two outputs.\n",
    "The outputs are to be limited to and continuous over the\n",
    "range 0 to 1. What can you tell about the network architecture?\n",
    "Specifically:\n",
    "* How many neurons are required?\n",
    "* What are the dimensions of the weight matrix?\n",
    "* What kind of transfer functions could be used?\n",
    "* Is a bias required?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer**: \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "We have a classification problem with four classes of input vector. The four classes are : \n",
    "* class 1 : $x_1 = (1,1)$ and $x_2 = (1,2)$\n",
    "* class 2 : $x_3 = (2,-1)$ and $x_4 = (2,0)$\n",
    "* class 3 : $x_5 = (-1,2)$ and $x_6 = (-2,1)$\n",
    "* class 4 : $x_7 = (-1,-1)$ and $x_8 = (-2,-2)$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Design a neural network to solve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Train a perceptron network to solve this problem\n",
    "using the perceptron learning rule.\n",
    "\n",
    "_Tip: be careful of the size of your matrix when you make a product. Here is an example of product:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B = np.array([[1,1],[4,-1]]) # it is a matrix\n",
    "print(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([1,1]) # it is a vector\n",
    "C = np.dot(B,x)  # it is a vector\n",
    "print(C)\n",
    "print(np.array([C])) #it is a matrix\n",
    "print(np.array([C]).T) #it is a column matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data : training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=[];T=[]\n",
    "X.append(np.array([1,1]))\n",
    "T.append(np.array([0,0]))\n",
    "X.append(np.array([1,2]))\n",
    "T.append(np.array([0,0]))\n",
    "X.append(np.array([2,-1]))\n",
    "T.append(np.array([0,1]))\n",
    "X.append(np.array([2,0]))\n",
    "T.append(np.array([0,1]))\n",
    "X.append(np.array([-1,2]))\n",
    "T.append(np.array([1,0]))\n",
    "X.append(np.array([-2,1]))\n",
    "T.append(np.array([1,0]))\n",
    "X.append(np.array([-1,-1]))\n",
    "T.append(np.array([1,1]))\n",
    "X.append(np.array([-2,-2]))\n",
    "T.append(np.array([1,1]))\n",
    "for i in range(len(X)):\n",
    "    T[i]=np.array([T[i]]).T\n",
    "    X[i]=np.array([X[i]]).T\n",
    "\n",
    "W = np.array([[1,0],[0,1]])\n",
    "b = np.array([[1,1]]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Weights after training : ' + str(W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Bias after training : ' + str(b))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
